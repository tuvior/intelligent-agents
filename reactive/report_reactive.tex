\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
% add other packages here

% put your group number and names in the author field
\title{\bf Exercise 2: A Reactive Agent for the Pickup and Delivery Problem}
\author{Group \textnumero1: Tobias Bordenca, Paul Nicolet}

% the report should not be longer than 3 pages

\begin{document}
\maketitle

\section{Problem Representation}

\subsection{Representation Description}
% describe how you design the state representation, the possible actions, the reward table and the probability transition table
\subsubsection{State Representation}
In order to take a decision for the next destination, it makes sense to take into account the current position and if there is an available task or not. This allows to clearly distinguish if it is worth it to take the task, and if not where to go next. In this direction, our internal state representation is the following: \textit{S = (Current city, Task destination)}. If the task destination is $\varnothing$, then the current city does not offer any task.

\subsubsection{Actions}
There are several possible actions depending on the state. 

If the current state contains a task, then it is either possible to take it and deliver it, or not take it and move to one of the neighbour city. It is important to note here that moving to each of the neighbouring cities is a different action to consider.

If the current state does not contain a task, then all the possible actions are moving to one of the neighbouring cites.

\subsubsection{Rewards}
Given a state and an action, the reward is computed as follows:
\begin{itemize}
	\item \textit{R((City A, $\varnothing$), (Move to B)) = - Cost(A, B)} if there is no task
	\item \textit{R((City A, City B), (Move to B)) = Gain(Task(A, B)) - Cost(A, B)} if there is a task and we decide to deliver it
	\item \textit{R((City A, City B), (Move to C)) =  - Cost(A, C)} if there is a task and we decide to not deliver it
\end{itemize}

\subsubsection{Transitions}
Given a current state, an action and the next state, the transition probabilities are computed as follows, considering that \textit{1 - p(A) = (1 - p(A, B)) * (1 - p(A, C)) * ...} is equal to the probability that there is no task at city A:
\begin{itemize}
	\item \textit{T((City A, $\varnothing$), (Move to B), (City B, $\varnothing$)) = (1 - p(A)) * (1 - p(B))}, if B is neighbour
	\item \textit{T((City A, City B), (Move to B), (City B, $\varnothing$)) = p(A, B) * (1 - p(B))}, when we pick the task
	\item \textit{T((City A, City B), (Move to B), (City B, City C)) = p(A, B) * p(B, C)}, when we pick the task
	\item \textit{T((City A, City B), (Move to C), (City C, $\varnothing$)) = p(A, B) * (1 - p(C))}, when we don't pick the task, if C is neighbour
	\item \textit{T((City A, City B), (Move to C), (City C, City D)) = p(A, B) * p(C, D)}, when we don't pick the task, if C is neighbour
	\item \textit{T((City A, *), (Move to C), (City C, *)) = 0} if C is not neighbour, or if the action does does not correspond to the arrival city
\end{itemize}

\subsection{Implementation Details}
% describe the implementation details of the representations above and the implementation details of the reinforcement learning algorithm you implemented
\subsubsection{Internal Representation}
One important thing to note in our representation is that we represent \textit{Actions} as \textit{Cities}, as the corresponding action, be it \textit{Pickup} or \textit{Move} can be inferred by the state. Doing this allows to create more direct mappings in our data structures.
\\
\\
The main class is the \textit{State class}, containing the following fields: 
\begin{itemize}
	\item \textit{currentCity} the state current city
	\item \textit{taskDestination} the available task destination city, or \textit{null} if no task available
	\item \textit{reward} the reward table for the state as \textit{Map<City, Double>} mapping an action to the reward
	\item \textit{transitionTable} the transition table for the state as \textit{Map<City, HashMap<State, Double>{}>} mapping an action to the next state and its corresponding probability
\end{itemize}

\subsubsection{Reinforcement Learning Algorithm}

The learning algorithm works in 3 steps.

In the first step a list of all possible states is generated, each state will be populated itself with the list of states, this allows the transition tables to contain pointers to other state, making it so that no lookup will be needed in the next steps.

The second step will calculate the \textit{Value} for each state iteratively. We traverse the state list in a nested loop, updating each state's value with the discounted expected value obtained by performing a transition. This operation is performed until all values reach a stable point.

In the last step an optimal strategy will be generated based on the previously computed values. Once more we iterate through all states and for each of these we map a \textit{City}, which represents the best action that can be taken in the current state to maximise profit.
The result of this final step is a strategy of the form \textit{Map<State, City>}, which can be used by the agent to fetch the next action to be taken directly.


\section{Results}
% in this section, you describe several results from the experiments with your reactive agent

\subsection{Experiment 1: Discount factor}
% the purpose of this experiment is to understand how the discount factor influences the result

\subsubsection{Setting}
% you describe how you perform the experiment (you also need to specify the configuration used for the experiment)
For a given topology and convergence threshold, we change the value of the discount factor in order to understand the influence on the computations and results. We try the following values: 0.9, 0.5 and 0.1.

\subsubsection{Observations}
% you describe the experimental results and the conclusions you inferred from these results
The results are quite surprising. Indeed, playing with the discount factor does not induce any significant changes in the results. The average profit converges at around 38000 for three cases and we cannot notice where does the discount factor play a from the results. We can explain it because the optimal solution is reached very quickly in any case.

However, we can see that the convergence changes a little. The value iteration algorithm takes longer to converge with a learning rate close to 1, and so fewer iterations to get an optimal value with a small discount. This is due to the fact that we include more learning in the iterations, and then values tend to vary a bit more before reaching an optimal solution. However, we are still guaranteed in any case to get a bounded error with any value of the learning rate.

\subsection{Experiment 2: Comparisons with dummy agents}
% you compare the results of your agent with two dummy agents: the random agent that was already given in the starter files and another dummy agent that you define and create. You should report the results from the simulations using the topologies given in the starter files and optionally, additional topologies that you create.

\subsubsection{Setting}
This experiment is done using $0.99$ as discount factor for the reactive agent.
The second dummy agent we will be testing works as follows: if a \textit{Task} is available, take it, otherwise move to the nearest \textit{City}.
Observations will be based on the average \textit{Reward} after 2000 actions, using the same seed for all 3 agents.
\subsubsection{Observations}
% elaborate on the observed results

\textbf{Random Agent}:
\textit{The total profit after $2000$ actions is $63,553,861$ (average profit: $31,776.9305$)}\\
\textbf{Yes Agent}:
\textit{The total profit after $2000$ actions is $73,620,955$ (average profit: $36,810.4775$)}\\
\textbf{Reactive Agent}:
\textit{The total profit after $2000$ actions is $76,526,797$ (average profit: $38,263.3985$)}\\

As it would be expected, the agent that takes actions at random comes out as the worst, as nothing is made to optimize decisions. The small gap between the other two agents is interesting, the second dummy agent is minimizing loss by always taking short movements when non delivering a \textit{Task}, and "maximising" profit by always taking a \textit{Task} when possible. 

This confirms to us that having an agent that sometimes refuses a \textit{Task} by following a prebuilt strategy will indeed allow for an improved performance over just maximizing instantaneous profit.

\end{document}