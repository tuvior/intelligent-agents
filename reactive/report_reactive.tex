\documentclass[11pt]{article}

\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
% add other packages here

% put your group number and names in the author field
\title{\bf Exercise 2: A Reactive Agent for the Pickup and Delivery Problem}
\author{Group \textnumero1: Tobias Bordenca, Paul Nicolet}

% the report should not be longer than 3 pages

\begin{document}
\maketitle

\section{Problem Representation}

\subsection{Representation Description}
% describe how you design the state representation, the possible actions, the reward table and the probability transition table
\subsubsection{State Representation}
In order to take a decision for the next destination, it makes sense to take into account the current position and if there is an available task or not. This allows to clearly distinguish if it is worth it to take the task, and if not where to go next. In this direction, our internal state representation is the following: \textit{S = (Current city, Task destination)}. If the task destination is $\varnothing$, then the current city does not offer any task.

\subsubsection{Actions}
There are several possible actions depending on the state. 

If the current state contains a task, then it is either possible to take it and deliver it, or not take it and move to one of the neighbour city. It is important to note here that moving to each of the neighbouring cities is a different action to consider.

If the current state does not contain a task, then all the possible actions are moving to one of the neighbouring cites.

\subsubsection{Rewards}
Given a state and an action, the reward is computed as follows:
\begin{itemize}
	\item \textit{R((City A, $\varnothing$), (Move to B)) = - Cost(A, B)} if there is no task
	\item \textit{R((City A, City B), (Move to B)) = Gain(Task(A, B)) - Cost(A, B)} if there is a task and we decide to deliver it
	\item \textit{R((City A, City B), (Move to C)) =  - Cost(A, C)} if there is a task and we decide to not deliver it
\end{itemize}

\subsubsection{Transitions}
Given a current state, an action and the next state, the transition probabilities are computed as follows, considering that \textit{1 - p(A) = (1 - p(A, B)) * (1 - p(A, C)) * ...} is equal to the probability that there is no task at city A:
\begin{itemize}
	\item \textit{T((City A, $\varnothing$), (Move to B), (City B, $\varnothing$)) = (1 - p(A)) * (1 - p(B))}, if B is neighbour
	\item \textit{T((City A, City B), (Move to B), (City B, $\varnothing$)) = p(A, B) * (1 - p(B))}, when we pick the task
	\item \textit{T((City A, City B), (Move to B), (City B, City C)) = p(A, B) * p(B, C)}, when we pick the task
	\item \textit{T((City A, City B), (Move to C), (City C, $\varnothing$)) = p(A, B) * (1 - p(C))}, when we don't pick the task, if C is neighbour
	\item \textit{T((City A, City B), (Move to C), (City C, City D)) = p(A, B) * p(C, D)}, when we don't pick the task, if C is neighbour
	\item \textit{T((City A, *), (Move to C), (City C, *)) = 0} if C is not neighbour, or if the action does does not correspond to the arrival city
\end{itemize}

\subsection{Implementation Details}
% describe the implementation details of the representations above and the implementation details of the reinforcement learning algorithm you implemented
\subsubsection{Internal Representation}
One important thing to note in our representation is that we represent \textit{Actions} as \textit{Cities}, as the corresponding action, be it \textit{Pickup} or \textit{Move} can be inferred by the state. Doing this allows to create more direct mappings in our data structures.
\\
\\
The main class is the \textit{State class}, containing the following fields: 
\begin{itemize}
	\item \textit{currentCity} the state current city
	\item \textit{taskDestination} the available task destination city, or \textit{null} if no task available
	\item \textit{reward} the reward table for the state as \textit{Map<City, Double>} mapping an action to the reward
	\item \textit{transitionTable} the transition table for the state as \textit{Map<City, HashMap<State, Double>{}>} mapping an action to the next state and its corresponding probability
\end{itemize}

\subsubsection{Reinforcement Learning Algorithm}




\section{Results}
% in this section, you describe several results from the experiments with your reactive agent

\subsection{Experiment 1: Discount factor}
% the purpose of this experiment is to understand how the discount factor influences the result

\subsubsection{Setting}
% you describe how you perform the experiment (you also need to specify the configuration used for the experiment)

\subsubsection{Observations}
% you describe the experimental results and the conclusions you inferred from these results

\subsection{Experiment 2: Comparisons with dummy agents}
% you compare the results of your agent with two dummy agents: the random agent that was already given in the starter files and another dummy agent that you define and create. You should report the results from the simulations using the topologies given in the starter files and optionally, additional topologies that you create.

\subsubsection{Setting}
% you describe how you perform the experiment and you describe the dummy agent you created (you also need to specify the configuration used for the experiment)

\subsubsection{Observations}
% elaborate on the observed results

\vdots

\subsection{Experiment n}
% other experiments you would like to present

\subsubsection{Setting}

\subsubsection{Observations}

\end{document}